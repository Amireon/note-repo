## 核心概念


## 文档分析
### 内置分析器
Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：
```
"Set the shape to semi-transparent by calling set_trans(5)"
```
- 标准分析器
标准分析器是Elasticsearch 默认使用的分析器。它是分析各种语言文本最常用的选择。它根据Unicode 联盟定义的单词边界划分文本。删除绝大部分标点。最后，将词条小写。它会产生：
```
set, the, shape, to, semi, transparent, by, calling, set_trans, 5
```

- 简单分析器
简单分析器在任何不是字母的地方分隔文本，将词条小写，它会产生：
```text
set, the, shape, to, semi, transparent, by, calling, set, trans
```

- 空格分析器
空格分析器在空格的地方划分文本，它会产生：
```text
Set, the, shape, to, semi-transparent, by, calling, set_trans(5)
```

- 语言分析器
特定语言分析器可用于很多语言。它们可以考虑指定语言的特点。例如，英语分析器附带了一组英语无用词（常用单词，例如`and`或者`the` ,它们对相关性没有多少影响），它们会被删除。由于理解英语语法的规则，这个分词器可以提取英语单词的词干。

英语分词器会产生下面的词条：
```
set, shape, semi, transpar, call, set_tran, 5
```
注意看`transparent`、`calling`和 `set_trans`已经变为词根格式。

### 分析器使用场景
当我们索引一个文档，它的全文已被分析成词条用来创建倒排索引。
当我们在全文域搜素时，需要将查询字符串通过相同的分析过程以保证我们搜索的词条格式与索引中的词条格式一致。

全文查询，理解每个域是如何被定义的，：
- 当你查询一个全文域时，会对查询字符串应用相同的分析器，以产生正确的搜索词条列表
- 当你查询一个精确值域时，不会分析查询字符串，而是搜索你指定的精确值。

### 测试分析器
有些时候很难理解分词的过程和实际被存储到索引中的词条，为了理解发生了什么，可以使用`analyze API`来看文本是如何被分析的。

在消息体里，指定分析器和要分析的文本：
```json
# GET http://localhost:9200/_analyze
{ 
	"analyzer": "standard", 
	"text": "Text to analyze" 
}
```
结果中每个元素代表一个单独的词条：
```json
{
    "tokens": [
        {
            "token": "text", 
            "start_offset": 0, 
            "end_offset": 4, 
            "type": "<ALPHANUM>", 
            "position": 1
        }, 
        {
            "token": "to", 
            "start_offset": 5, 
            "end_offset": 7, 
            "type": "<ALPHANUM>", 
            "position": 2
        }, 
        {
            "token": "analyze", 
            "start_offset": 8, 
            "end_offset": 15, 
            "type": "<ALPHANUM>", 
            "position": 3
        }
    ]
}
```

- *token*: 实际存储到索引中的词条
- `start_offset`和`end_offset`：字符在原始字符串中的位置
- `position`：词条在原始文本中出现的位置

### 指定分析器
当Elasticsearch在文档中检测到一个新的字符串域，它会自动设置其为一个全文字符串域，使用标准分析器对它进行分析。
有时候想要一个字符串域就是一个字符串域，不使用分析，直接索引传入的精确值，例如用户 ID 或者一个内部的状态域或标签。要做到这一点，我们必须手动指定这些域的映射。

### IK分词器
#### 中文分词
首先通过 Postman 发送 GET 请求查询分词效果
```json
# GET http://localhost:9200/_analyze
{
	"text":"测试单词"
}
```
ES 的默认分词器无法识别中文中测试、 单词这样的词汇，而是简单的将每个字拆完分为一个词。
```json
{
    "tokens": [
        {
            "token": "测", 
            "start_offset": 0, 
            "end_offset": 1, 
            "type": "<IDEOGRAPHIC>", 
            "position": 0
        }, 
        {
            "token": "试", 
            "start_offset": 1, 
            "end_offset": 2, 
            "type": "<IDEOGRAPHIC>", 
            "position": 1
        }, 
        {
            "token": "单", 
            "start_offset": 2, 
            "end_offset": 3, 
            "type": "<IDEOGRAPHIC>", 
            "position": 2
        }, 
        {
            "token": "词", 
            "start_offset": 3, 
            "end_offset": 4, 
            "type": "<IDEOGRAPHIC>", 
            "position": 3
        }
    ]
}
```
结果显然不符合我们的使用要求，所以我们需要下载 ES 对应版本的中文分词器：[IK 中文分词器下载网址](https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.8.0)，将解压后的后的文件夹放入 ES 根目录下的 plugins 目录下，重启 ES 即可使用。

我们这次加入新的查询参数"analyzer":“ik_max_word”。
```json
#GET http://localhost:9200/_analyze
{
	"text":"测试单词",
	"analyzer":"ik_max_word"
}
```
-   `ik_max_word`：会将文本做最细粒度的拆分。
-   `ik_smart`：会将文本做最粗粒度的拆分。

使用中文分词后的结果为：
```json
{
    "tokens": [
        {
            "token": "测试", 
            "start_offset": 0, 
            "end_offset": 2, 
            "type": "CN_WORD", 
            "position": 0
        }, 
        {
            "token": "单词", 
            "start_offset": 2, 
            "end_offset": 4, 
            "type": "CN_WORD", 
            "position": 1
        }
    ]
}
```

#### 扩展词汇
ES 中也可以进行扩展词汇，首先查询:
```json
#GET http://localhost:9200/_analyze

{
    "text":"弗雷尔卓德",
    "analyzer":"ik_max_word"
}
```
分析结果如下：
```json
{
    "tokens": [
        {
            "token": "弗",
            "start_offset": 0,
            "end_offset": 1,
            "type": "CN_CHAR",
            "position": 0
        },
        {
            "token": "雷",
            "start_offset": 1,
            "end_offset": 2,
            "type": "CN_CHAR",
            "position": 1
        },
        {
            "token": "尔",
            "start_offset": 2,
            "end_offset": 3,
            "type": "CN_CHAR",
            "position": 2
        },
        {
            "token": "卓",
            "start_offset": 3,
            "end_offset": 4,
            "type": "CN_CHAR",
            "position": 3
        },
        {
            "token": "德",
            "start_offset": 4,
            "end_offset": 5,
            "type": "CN_CHAR",
            "position": 4
        }
    ]
}
```
仅仅可以得到每个字的分词结果，我们需要做的就是使分词器识别到弗雷尔卓德也是一个词语：
1.  首先进入 ES 根目录中的 plugins 文件夹下的 ik 文件夹，进入 config 目录，创建 custom.dic文件，写入“弗雷尔卓德”。
2.  同时打开 IKAnalyzer.cfg.xml 文件，将新建的 custom.dic 配置其中。
3.  重启 ES 服务器 。
` IKAnalyzer.cfg.xml`文件核心内容：
```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 -->
	<entry key="ext_dict">custom.dic</entry>
	 <!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords"></entry>
	<!--用户可以在这里配置远程扩展字典 -->
	<!-- <entry key="remote_ext_dict">words_location</entry> -->
	<!--用户可以在这里配置远程扩展停止词字典-->
	<!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
```

扩展后再次查询：
```json
# GET http://localhost:9200/_analyze
{
	"text":"测试单词",
	"analyzer":"ik_max_word"
}
```
返回结果如下：
```json
{
    "tokens": [
        {
            "token": "弗雷尔卓德",
            "start_offset": 0,
            "end_offset": 5,
            "type": "CN_WORD",
            "position": 0
        }
    ]
}
```

### 自定义分析器
虽然Elasticsearch带有一些现成的分析器，然而在分析器上Elasticsearch真正的强大之处在
于，你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器
来创建自定义的分析器。

在分析与分析器我们说过，一个分析器就是在一个包里面组合了三种函数的一个包装器，三种函
数按照顺序被执行：

**1. 字符过滤器**
**字符过滤器用来整理一个尚未被分词的字符串。**
例如，如果我们的文本是HTML格式的，它会包含像`<p>`或者`<div>`这样的HTML标签，这些标签
是我们不想索引的。我们可以使用html清除字符过滤器来移除掉所有的HTML标签，并且像把
`&Aacute;`转换为相对应的Unicode字符`Á `这样，转换HTML实体。
一个分析器可能有0个或者多个字符过滤器。

**2.分词器**
一个分析器必须有一个唯一的分析器，**分词器把字符串分解成单个词条或者词汇单元。**
标准分析器里使用的标准分词器把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号。
还有其他不同行为的分词器存在：
- 关键词分词器完整地输出接收到的同样的字符串，并不做任何分词
- 空格分词器只根据空格分割文本
- 正则分词器根据匹配正则表达式来分割文本

**3. 词单元过滤器**
经过分词，作为结果的词单元流会按照指定的顺序通过指定的词单元过滤器。
**词单元过滤器可以修改、添加或者移除词单元。**
我们已经提到过lowercase和stop词过滤器，但是在Elasticsearch 里面还有很多可供选择的词单元过滤器：
- 词干过滤器把单词遏制为词干
- ascii_folding过滤器移除变音符，把一个像"très”这样的词转换为“tres”
- ngram和 edge_ngram词单元过滤器可以产生适合用于部分匹配或者自动补全的词单元

例
接下来，我们看看如何创建自定义的分析器：
```json
#PUT http://localhost:9200/my_index

{
    "settings": {
        "analysis": {
            "char_filter": {
                "&_to_and": {
                    "type": "mapping", 
                    "mappings": [
                        "&=> and "
                    ]
                }
            }, 
            "filter": {
                "my_stopwords": {
                    "type": "stop", 
                    "stopwords": [
                        "the", 
                        "a"
                    ]
                }
            }, 
            "analyzer": {
                "my_analyzer": {
                    "type": "custom", 
                    "char_filter": [
                        "html_strip", 
                        "&_to_and"
                    ], 
                    "tokenizer": "standard", 
                    "filter": [
                        "lowercase", 
                        "my_stopwords"
                    ]
                }
            }
        }
    }
}

```

索引被创建以后，使用 analyze API 来 测试这个新的分析器：
```json
# GET http://127.0.0.1:9200/my_index/_analyze
{
    "text":"The quick & brown fox",
    "analyzer": "my_analyzer"
}

```

返回结果为：
```json
{
    "tokens": [
        {
            "token": "quick",
            "start_offset": 4,
            "end_offset": 9,
            "type": "<ALPHANUM>",
            "position": 1
        },
        {
            "token": "and",
            "start_offset": 10,
            "end_offset": 11,
            "type": "<ALPHANUM>",
            "position": 2
        },
        {
            "token": "brown",
            "start_offset": 12,
            "end_offset": 17,
            "type": "<ALPHANUM>",
            "position": 3
        },
        {
            "token": "fox",
            "start_offset": 18,
            "end_offset": 21,
            "type": "<ALPHANUM>",
            "position": 4
        }
    ]
}

```


## 文档控制
